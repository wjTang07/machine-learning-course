{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing ensemble methods\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "### GRA 4160\n",
    "### Predictive modelling with machine learning\n",
    "\n",
    "#### Lecturer: Vegard H. Larsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble methods are a type of machine learning technique that combine the predictions of multiple models to make more accurate predictions than any individual model could. These methods are particularly useful in situations where the individual models have high variance or make strong, complex predictions. There are several types of ensemble methods, including boosting, bagging, and bootstrapped ensembles.\n",
    "\n",
    "One popular type of ensemble method is boosting, in which a series of weak models are trained sequentially, with each model attempting to correct the mistakes of the previous model. The final prediction is made by combining the predictions of all the models in the ensemble. Boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "Another type of ensemble method is bagging, in which a group of models are trained independently on different random subsets of the training data. The final prediction is made by averaging the predictions of all the models in the ensemble. Bagging algorithms include Random Forests and Extra Trees. Ensemble methods have been successful in a wide range of applications, including image classification and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "\n",
    "Combines multiple classifiers and uses a voting scheme to make predictions. The voting scheme can be either **hard** or **soft**, depending on how the final prediction is made.\n",
    "\n",
    "In a hard voting scheme, the final prediction is the mode of the predictions of the individual classifiers.\n",
    "In other words, each classifier casts a \"vote\" for its predicted class, and the class that receives the most votes is chosen as the final prediction.\n",
    "This is equivalent to a simple majority vote.\n",
    "\n",
    "In a soft voting scheme, the final prediction is the class with the highest probability of being predicted by the individual classifiers.\n",
    "In other words, each classifier produces a set of probabilities for each class, and the probabilities are averaged across all the classifiers.\n",
    "The class with the highest average probability is chosen as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset and split it into training and testing sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=2)\n",
    "\n",
    "# Define the base classifiers\n",
    "clf1 = LogisticRegression(random_state=10, solver='lbfgs', max_iter=1000)\n",
    "clf2 = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the VotingClassifier with hard voting\n",
    "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2)], voting='soft')\n",
    "\n",
    "# Train the LogisticRegression and DecisionTree\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LogisticRegression and RandomForestClassifier on the testing set\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "# Train the VotingClassifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the VotingClassifier on the testing set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy logistic regression: {accuracy1:.3f}')\n",
    "print(f'Accuracy decision tree: {accuracy2:.3f}')\n",
    "print(f'Accuracy voting classifier: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week and strong learners\n",
    "\n",
    "A weak learner is a model that performs only slightly better than random guessing.\n",
    "For example, a decision tree with only one split or a linear regression model with a low degree polynomial can be considered weak learners.\n",
    "Although weak learners may not perform well individually, they can be combined in various ways to create a strong learner.\n",
    "\n",
    "A strong learner, on the other hand, is a model that can make accurate predictions on a given task with high confidence.\n",
    "A strong learner can be created by combining multiple weak learners using ensemble methods such as boosting, bagging, and stacking.\n",
    "\n",
    "In boosting, weak learners are trained sequentially, with each subsequent learner focused on the samples that the previous learner got wrong.\n",
    "By doing so, boosting can increase the accuracy of the model and create a strong learner from a collection of weak learners.\n",
    "Examples of boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "In bagging, weak learners are trained independently on different subsets of the data, and their predictions are aggregated using a voting scheme or an average.\n",
    "Bagging can reduce the variance of the model and create a strong learner from a collection of unstable weak learners.\n",
    "Examples of bagging algorithms include Bagging classifier (today) Random Forest and Extra Trees (will be covered next lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging)\n",
    "\n",
    "- Bagging stands for Bootstrap Aggregation\n",
    "- It is an ensemble technique that combines multiple models trained on different subsets of the training data\n",
    "- Bagging reduces overfitting by aggregating the results of many base models trained on different subsets of the training data, leading to a more generalized model.\n",
    "- In bagging, each model is trained independently, and the final prediction is the average (in regression) or majority vote (in classification) of the predictions of the individual models.\n",
    "- One of the most popular bagging algorithms is the Random Forest algorithm (will be covered in the next lecture), which builds a collection of decision trees using random subsets of the features and training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Bootstrap works by creating multiple samples from the original data by randomly sampling with replacement.\n",
    "In each bootstrap sample, some data points are selected multiple times, while others are not selected at all.\n",
    "This results in a new dataset that has the same size as the original dataset but with some variability in the data points.\n",
    "\n",
    "By creating multiple bootstrap samples and training the model on each sample, we can estimate the variability of the model's performance and parameter estimates, and compute confidence intervals for the model's predictions.\n",
    "This is particularly useful when the sample size is small, or when the distribution of the data is unknown or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Breast Cancer with Bagging\n",
    "\n",
    "Here we will classify breast cancer based on a dataset of patient information.\n",
    "The dataset contains information on patient features such as their age, tumor size, and number of positive lymph nodes, as well as whether the patient's cancer has recurred.\n",
    "We train a classifier to predict whether a new patient's cancer is likely to recur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the Bagging Classifier\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of the BaggingClassifier\n",
    "bag_clf = BaggingClassifier(estimator=tree_clf, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier on the training set\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Classifier\n",
    "# Make predictions on the testing set\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the confusion matrix of the classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate the classification report of the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Hyperparameters\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of the BaggingClassifier with different hyperparameters\n",
    "bag_clf1 = BaggingClassifier(estimator=tree_clf, n_estimators=100, max_samples=10, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf2 = BaggingClassifier(estimator=tree_clf, n_estimators=200, max_samples=50, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf3 = BaggingClassifier(estimator=tree_clf, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf4 = BaggingClassifier(estimator=tree_clf, n_estimators=1000, max_samples=200, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train each BaggingClassifier on the training set\n",
    "bag_clf1.fit(X_train, y_train)\n",
    "bag_clf2.fit(X_train, y_train)\n",
    "bag_clf3.fit(X_train, y_train)\n",
    "bag_clf4.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of each BaggingClassifier on the testing set\n",
    "print(\"Bagging Classifier 1:\")\n",
    "print(\"Accuracy:\", bag_clf1.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 2:\")\n",
    "print(\"Accuracy:\", bag_clf2.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 3:\")\n",
    "print(\"Accuracy:\", bag_clf3.score(X_test, y_test))\n",
    "print(\"Bagging Classifier 4:\")\n",
    "print(\"Accuracy:\", bag_clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Feature importances\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the feature importances\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "plt.barh(range(data.data.shape[1]), feature_importances)\n",
    "plt.yticks(range(data.data.shape[1]), data.feature_names)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
